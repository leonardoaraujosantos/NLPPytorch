{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Before we start feeding data to our recurrent models we need first to convert the words to a numeric representation (feature), that also conveys some meaning. There are 2 main options to do the job:\n",
    "* [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "* [Word2Vec](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "\n",
    "#### References\n",
    "* [Pytorch tutorials](http://pytorch.org/tutorials/)\n",
    "* [Pytorch text glove tutorial](https://github.com/spro/practical-pytorch/blob/master/glove-word-vectors/glove-word-vectors.ipynb)\n",
    "* [Pytorch word embeddings tutorial](http://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py)\n",
    "* [The Amazing power of word vectors](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)\n",
    "* [Glove global vectors](https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/)\n",
    "* [NLTK Python NLP Toolkit](http://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a glove word to vector converter and define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word vectors from ./glove.6B.100d.pt\n",
      "Loaded 400000 words\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Pytorch extension for NLP (Datasets, and some tools like word2vec and glove)\n",
    "from torchtext.vocab import load_word_vectors\n",
    "\n",
    "# Get a dictionary of words (word to vec) with word vector size of 100 dimensions\n",
    "# It will download around 800Mb if necessary\n",
    "wv_dict, wv_arr, wv_size = load_word_vectors('.', 'glove.6B', 100)\n",
    "print('Loaded', len(wv_arr), 'words')\n",
    "\n",
    "# Define simple function to get the word vector (using returned dictionaries)\n",
    "def get_embeddings(word):\n",
    "    return wv_arr[wv_dict[word.lower()]]\n",
    "\n",
    "# Get closest n words\n",
    "def get_closest(d, n=10):\n",
    "    all_dists = [(w, torch.dist(d, get_embeddings(w))) for w in wv_dict]\n",
    "    return sorted(all_dists, key=lambda t: t[1])[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec_europa = get_embeddings('europa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing some vector Arithimetic\n",
    "One of the coolest features of a well trained word to vector model is that we can use arithimetic with the semantic vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In the form w1 - w2 + w3 : ?\n",
    "def analogy(w1, w2, w3, n=5, filter_given=True):\n",
    "    print('\\n[%s : %s :: %s : ?]' % (w1, w2, w3))\n",
    "   \n",
    "    # w2 - w1 + w3 = w4\n",
    "    closest_words = get_closest(get_embeddings(w1) - get_embeddings(w2) + get_embeddings(w3))\n",
    "    \n",
    "    # Optionally filter out given words\n",
    "    if filter_given:\n",
    "        closest_words = [t for t in closest_words if t[0] not in [w1, w2, w3]]\n",
    "    \n",
    "    return closest_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[king : man :: woman : ?]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('queen', 4.081078406285764),\n",
       " ('monarch', 4.642907605578163),\n",
       " ('throne', 4.905500998697052),\n",
       " ('elizabeth', 4.921558575828151),\n",
       " ('prince', 4.981146936056392),\n",
       " ('daughter', 4.985715105960012),\n",
       " ('mother', 5.064087418704118),\n",
       " ('cousin', 5.077497332002661),\n",
       " ('princess', 5.07868555349649)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('king','man', 'woman')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
